{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb3b79e",
   "metadata": {},
   "source": [
    "From https://www.analyticsvidhya.com/blog/2021/10/making-natural-language-processing-easy-with-textblob/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5165bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bed557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence = TextBlob(\"I am reading a blog post on AnalyticsVidhya. I am loving it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4743563",
   "metadata": {},
   "source": [
    "# POS Tagging using Textblob\n",
    "The technique of assigning one of the parts of speech to a given word is known as (PoS) tagging. With POS or Part-of-speech tagging, we can list the part-of-speech tags through the tags property. It’s also known as “point-of-sale” tagging in the long-form. In layman’s terms, POS tagging is the task of labeling each word in a phrase with its proper part of speech. Nouns, verbs, adverbs, adjectives, pronouns, conjunctions, and their subcategories are all known parts of speech. Rule-based POS tagging, stochastic POS tagging, and transformation-based tagging are the most common types of POS tagging.\n",
    "\n",
    "Rule-based POS tagging is one of the most ancient tagging methods. To find possible tags for each word, rule-based taggers consult a dictionary or lexicon. Rule-based taggers utilize hand-written rules to choose the correct tag if the word has more than one possible tag. The linguistic properties of a word, as well as its preceding and succeeding words, can be analyzed in rule-based tagging to disambiguate it.\n",
    "\n",
    "Stochastic POS Tagging is another tagging approach. Stochastic refers to a model that includes frequency or probability (statistics). Stochastic tagger refers to a variety of alternative methods to the problem of part-of-speech tagging. Word frequency and tag sequence are applied in the simplest stochastic tagger.\n",
    "\n",
    "Transformation-based tagging is also referred to as Brill tagging. It’s a transformation-based learning (TBL) example, which is a rule-based system for automatically labeling POS to given text. TBL converts one state to another using transformation rules, allowing us to have linguistic knowledge in a legible fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "273ee758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('reading', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('blog', 'NN'),\n",
       " ('post', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('AnalyticsVidhya', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('loving', 'VBG'),\n",
       " ('it', 'PRP')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f526df",
   "metadata": {},
   "source": [
    "# Noun Phrases using Textblob\n",
    "The Merriam-webster defines Noun as: Any member of a class of words that typically can be combined with determiners to serve as the subject of a verb, can be interpreted as singular or plural, can be replaced with a pronoun, and refer to an entity, quality, state, action, or concept.\n",
    "Let’s say we want to extract the noun phrases in our sentences. This can easily be done using the noun phrases property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07716131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['blog post', 'analyticsvidhya'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdebe66",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Textblob\n",
    "Sentiment Analysis can assist us in determining the mood and feelings of the general public as well as obtaining useful information about the setting. Sentiment Analysis is the process of assessing data and categorizing it according to the needs.\n",
    "\n",
    "The polarity and subjectivity of a statement are returned by TextBlob. The range of polarity is [-1,1], with -1 indicating a negative sentiment and 1 indicating a positive sentiment. Negative words are used to change the polarity of a sentence. Semantic labels in TextBlob aid in fine-grained analysis. Emoticons, exclamation marks, and emojis, for example. subjectivity falls under the numeric range of [0,1]. The degree of personal opinion and factual information in a text is measured by subjectivity. Because of the text’s heightened subjectivity, it contains personal opinion rather than factual information. There’s one more setting in TextBlob: intensity. The ‘intensity’ is used by TextBlob to calculate subjectivity. The intensity of a word influences whether it modifies the next word. Adverbs are used as modifiers in English.\n",
    "\n",
    "By providing an input sentence, the TextBlob’s sentiment property returns a named tuple with polarity and subjectivity scores. The polarity score ranges from -1.0 to 1.0 and the subjectivity ranges from 0.0 to 1.0 where 0.0 is an objective statement and 1 is a subjective statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8260d0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.75, subjectivity=0.95)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c10496",
   "metadata": {},
   "source": [
    "# Tokenization using Textblob\n",
    "Up next is tokenization. In any NLP pipeline, tokenization is considered to be the first step of the pipeline process. A tokenizer breaks down unstructured data and natural language text into chunks of information that can be considered discrete elements. The document’s token occurrences can be used to generate a vector that reflects the document. An unstructured string (text document) is turned into a numerical data structure suitable for machine learning in a matter of seconds. They can also be utilized to direct a computer’s helpful operations and responses. They could potentially be used as features in a machine learning pipeline to prompt more complex decisions or actions.\n",
    "\n",
    "Recent deep learning-powered NLP algorithms have interpreted tokens in the context in which they appear, even in very extensive contexts. Because the system can infer the “meaning” of unusual tokens based on their context, this ability mitigates the “Heteronyms” problem and makes NLP systems more robust in the face of rare tokens. The way we tokenize text has altered as a result of these relatively new capabilities in the realm of NLP. A pipeline approach is commonly used to tokenize non-deep learning systems. After separating the text into token candidates (by splitting on white spaces or using more complex heuristics), related tokens are merged and noisy tokens are removed.\n",
    "\n",
    "Modern tokenization systems, such as the Sentence piece or BPE algorithms, split and merge tokens into more intricate forms, and are referred to as subword tokenizers. BPE, for example, is a tokenization technique that allows for an unlimited vocabulary by expressing some tokens as pairs of tokens or more. One new approach is to almost completely avoid tokenization and run NLP algorithms at the character level. As a result, our models must process characters and understand their meanings in order to deal with considerably longer sequences. NLP at the character level allows us to avoid the nuances of tokenization and the errors that it can introduce, with sometimes astonishing results.\n",
    "\n",
    "We can easily break down the sentences into words or sentences. We have words and sentences properties for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b371db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['I', 'am', 'reading', 'a', 'blog', 'post', 'on', 'AnalyticsVidhya', 'I', 'am', 'loving', 'it'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e97d260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"I am reading a blog post on AnalyticsVidhya.\"),\n",
       " Sentence(\"I am loving it!\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c15ed93",
   "metadata": {},
   "source": [
    "# Word Inflection using Textblob\n",
    "We can easily singularize and pluralize the words with the help of “singularize” and “pluralize” properties respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f87af4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blogs'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence.words[4].pluralize() # the word \"blog\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715e8b3",
   "metadata": {},
   "source": [
    "# Lemmatization using Textblob\n",
    "Lemmatization is the process of aggregating together the derived forms of a word into a single element or item. The “lemmatize” property helps us achieve this functionality. In Natural Language Processing (NLP) and machine learning, lemmatization is one of the most used text techniques during the pre-processing phase. Stemming is a Natural Language Processing concept that is nearly comparable to this one. We strive to reduce a given term to its root word in both stemming and lemmatization. In the stemming process, the root word is termed a stem, and in the lemmatization process, it is called a lemma.\n",
    "\n",
    "Lemmatization has the advantage of being more precise. Lemmatization is important if you’re working with an NLP application like a chat bot or a virtual assistant when comprehending the meaning of the discussion is critical. However, this precision comes at a price.\n",
    "\n",
    "Because lemmatization entails determining a word’s meaning from a source such as a dictionary, it takes a long time. As a result, most lemmatization methods are slower than stemming techniques. Although there is a processing expense for lemmatization, computational resources are rarely a consideration in an ML challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a57ff1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'radius'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word(\"radii\")\n",
    "w.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "498dcc63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word(\"went\")\n",
    "w.lemmatize(\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb2a613",
   "metadata": {},
   "source": [
    "# Definition using Textblob\n",
    "TextBlob also offers the functionality of defining the given word. The property called “definitions” does the job for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d245a90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a shared on-line journal where people can post diary entries about their personal experiences and hobbies',\n",
       " 'read, write, or edit a shared on-line journal']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"blog\").definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ec204",
   "metadata": {},
   "source": [
    "## Synsets\n",
    "\n",
    "The “synsets” property returns a list of synset objects for a particular word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbf49310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('telephone.n.01'),\n",
       " Synset('phone.n.02'),\n",
       " Synset('earphone.n.01'),\n",
       " Synset('call.v.03')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = Word(\"phone\")\n",
    "word.synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d69a5",
   "metadata": {},
   "source": [
    "# Spelling Correction using Textblob\n",
    "The spell check operation is performed by the “correct()” method. It uses the classic approach of Peter Norvig’s “How to Write a Spelling Corrector?“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c801b776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I am not in danger. I am the danger.\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence = TextBlob(\"I am not in denger. I am the dyangr.\")\n",
    "my_sentence.correct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230d496",
   "metadata": {},
   "source": [
    "Similarly, the `spellcheck()` method returns a list of probably correct words along with the confidence in the form of a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6479c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pneumonia', 1.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('neumonia')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295652c1",
   "metadata": {},
   "source": [
    "# Word frequencies using Textblob\n",
    "The “word_counts” operation returns the number of counts of a particular word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3561bfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betty = TextBlob(\"Betty Botter bought some butter. But she said the Butter’s bitter. If I put it in my batter, it will make my batter bitter. But a bit of better butter will make my batter better.\")\n",
    "betty.word_counts['butter']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc2b7cc",
   "metadata": {},
   "source": [
    "To apply the case sensitiveness, we can apply the `.count(word, case_sensitive=True)` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "022c5dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betty.words.count('butter', case_sensitive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f6447b",
   "metadata": {},
   "source": [
    "# Parsing using TextBlob\n",
    "The term “parsing” comes from the Latin word “pars” (which means “part”). It is used to extract exact or dictionary meaning from a text. Syntactic analysis, or syntax analysis, is another name for it. Syntax analysis examines the text for meaning by comparing it to formal grammar rules. As a result, parsing, syntactic analysis, or syntax analysis can be defined as the process of analyzing strings of symbols in natural language that correspond to formal grammar rules.The “parse()” method parses the TextBlob by including the tags besides the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b844139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Betty/NNP/B-NP/O Botter/NNP/I-NP/O bought/VBD/B-VP/O some/DT/B-NP/O butter/NN/I-NP/O ././O/O\\nBut/CC/O/O she/PRP/B-NP/O said/VBD/B-VP/O the/DT/B-NP/O Butter/NN/I-NP/O ’/NN/I-NP/O s/PRP/I-NP/O bitter/JJ/B-ADJP/O ././O/O\\nIf/IN/B-PP/B-PNP I/PRP/B-NP/I-PNP put/VB/B-VP/O it/PRP/B-NP/O in/IN/B-PP/B-PNP my/PRP$/B-NP/I-PNP batter/NN/I-NP/I-PNP ,/,/O/O it/PRP/B-NP/O will/MD/B-VP/O make/VB/I-VP/O my/PRP$/B-NP/O batter/NN/I-NP/O bitter/JJ/B-ADJP/O ././O/O\\nBut/CC/O/O a/DT/B-NP/O bit/NN/I-NP/O of/IN/B-PP/B-PNP better/JJR/B-NP/I-PNP butter/NN/I-NP/I-PNP will/MD/B-VP/O make/VB/I-VP/O my/PRP$/B-NP/O batter/NN/I-NP/O better/JJR/B-ADJP/O ././O/O'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betty.parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3228a453",
   "metadata": {},
   "source": [
    "# Similarities of TextBlob with Python string\n",
    "TextBlobs are similar to Python strings. They can perform basic slicing operations just like the regular Python strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bc52861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Simple is better\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence = TextBlob(\"Simple is better than complex.\")\n",
    "my_sentence[0:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35486e4f",
   "metadata": {},
   "source": [
    "Apart from slicing, the “upper()” and “lower()” can also be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f534476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"SIMPLE IS BETTER THAN COMPLEX.\")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8ce6e",
   "metadata": {},
   "source": [
    "And just like the regular string, we can also perform the “find()” operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ce3f153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence.find(\"better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be022bcc",
   "metadata": {},
   "source": [
    "Textblobs and Python strings can easily be concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87db41b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Black and Blue\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = TextBlob(\"Black\")\n",
    "b = TextBlob(\"Blue\")\n",
    "a + ' and ' + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd59b3d",
   "metadata": {},
   "source": [
    "The object can also be formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2de73fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Black and Blue'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{0} and {1}\".format(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03ae4d",
   "metadata": {},
   "source": [
    "# N-gram using TextBlob\n",
    "An N-gram is simply the sequence of ‘n’ words. In the above example, the first statement “A cat is in the bag” is a 6-gram. Likewise, “Say my name” is 3-gram, and “Good luck” is 2-gram. N-grams are utilized for a wide range of tasks. When creating a language model, for example, n-grams are utilized to create not only unigram (single n-gram) models but also bigram (2-gram) and trigram (3-gram) or multiple models. Web scale n-gram models have been built by researchers for a number of applications including spelling correction, word breaking, and text summarization. Another application of n-grams is in the development of features for supervised Machine Learning models like SVMs and Naive Bayes, among others. Instead of using unigrams, the concept is to employ tokens like bigrams in the feature space.\n",
    "\n",
    "The n-gram operation (returning a list of tuples of n successive words) can also be easily performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7f300a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['How', 'many', 'roads']),\n",
       " WordList(['many', 'roads', 'should']),\n",
       " WordList(['roads', 'should', 'a']),\n",
       " WordList(['should', 'a', 'man']),\n",
       " WordList(['a', 'man', 'must']),\n",
       " WordList(['man', 'must', 'walk']),\n",
       " WordList(['must', 'walk', 'before']),\n",
       " WordList(['walk', 'before', 'we']),\n",
       " WordList(['before', 'we', 'can']),\n",
       " WordList(['we', 'can', 'call']),\n",
       " WordList(['can', 'call', 'him']),\n",
       " WordList(['call', 'him', 'a']),\n",
       " WordList(['him', 'a', 'man'])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob = TextBlob(\"How many roads should a man must walk before we can call him a man?\")\n",
    "bob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23790c14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3cc5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
